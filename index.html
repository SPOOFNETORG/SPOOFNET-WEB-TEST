<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Anti-Spoof Live</title>
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
<style>
  * {
    box-sizing: border-box;
    margin: 0;
    padding: 0;
  }

  body {
    text-align: center;
    font-family: Arial;
    overflow-x: hidden;
  }

  h2 {
    font-size: 30px;
    margin: 16px 0 6px;
  }

  .instructions {
    font-size: 15px;
    color: #444;
    max-width: 360px;
    margin: 8px auto 16px;
    line-height: 1.4;
    padding: 0 12px;
  }

  .video-wrapper {
    position: relative;
    /*
      Use 100vw minus a fixed gutter (12px each side).
      This guarantees it never touches screen edges on any phone,
      and caps at 400px on larger screens.
    */
    width: min(400px, calc(100vw - 24px));
    height: min(400px, calc(100vw - 24px));
    margin: 0 auto;
    overflow: hidden;
  }

  video {
    width: 100%;
    height: 100%;
    object-fit: cover;
    transform: scaleX(-1);
    display: block;
  }

  /*
    Oval overlay.

    FIX: gradient stops must be in strictly ascending order.
    transparent 90% → dark 91% gives a razor-sharp edge.

    ellipse 65% 80%  →  oval is 65% wide, 80% tall of the wrapper,
    leaving ~17.5% visible margin on left/right (~10% of total width).
  */
  .video-wrapper::after {
    content: "";
    position: absolute;
    inset: 0;
    pointer-events: none;
    background:
      radial-gradient(
        ellipse 35% 55% at center,
        transparent 0%,
        transparent 90%,
        rgba(0, 0, 0, 0.80) 91%
      );
  }
</style>
</head>
<body>

<h2>Live Anti-Spoof</h2>

<p class="instructions">
  • Ensure your face is clearly visible in good lighting (avoid harsh reflections).<br>
  • Align and fit your face tightly within the oval.
</p>

<div class="video-wrapper">
  <video id="video" autoplay playsinline></video>
</div>

<canvas id="canvas" width="224" height="224" style="display:none;"></canvas>
<p id="result" style="margin-top:12px;">Loading model...</p>

<script>
let session;

async function init() {
  session = await ort.InferenceSession.create("antispoof_model.onnx");
  document.getElementById("result").innerText = "Model loaded. Starting camera...";
  startCamera();
}

async function startCamera() {
  const video = document.getElementById("video");

  const stream = await navigator.mediaDevices.getUserMedia({
    video: { facingMode: "user" }
  });

  video.srcObject = stream;
  runLoop();
}

async function runLoop() {
  const video = document.getElementById("video");
  const canvas = document.getElementById("canvas");
  const ctx = canvas.getContext("2d");

  ctx.save();
  ctx.scale(-1, 1);
  ctx.drawImage(video, -224, 0, 224, 224);
  ctx.restore();

  const imageData = ctx.getImageData(0, 0, 224, 224).data;
  const floatData = new Float32Array(1 * 3 * 224 * 224);

  const mean = [0.485, 0.456, 0.406];
  const std  = [0.229, 0.224, 0.225];

  for (let i = 0; i < 224 * 224; i++) {
    const r = imageData[i * 4]     / 255;
    const g = imageData[i * 4 + 1] / 255;
    const b = imageData[i * 4 + 2] / 255;

    floatData[i]             = (r - mean[0]) / std[0];
    floatData[i + 224*224]   = (g - mean[1]) / std[1];
    floatData[i + 2*224*224] = (b - mean[2]) / std[2];
  }

  const tensor = new ort.Tensor("float32", floatData, [1, 3, 224, 224]);
  const results = await session.run({ input: tensor });

  const logit = results.output.data[0];
  const prob = 1 / (1 + Math.exp(-logit));

  document.getElementById("result").innerText =
    prob > 0.5 ? `REAL (${prob.toFixed(3)})` :
                 `ATTACK (${prob.toFixed(3)})`;

  requestAnimationFrame(runLoop);
}

init();
</script>

</body>
</html>
